{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pandas` DataFrames and tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we give some examples of how we can exploit [pandas](https://pandas.pydata.org/), a very versatile python library which is being used more and more also in HEP. \n",
    "In this ministage we are mainly using the `pandas` environment for our analysis. Here we have a first look at some of the main features of a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's import the dataset we are going to use. In this case we take a 300 GeV positrons sample from the October 2018 beam test. \n",
    "Normally HEP data come in the `.root` format, which was born to be directly usable with the [`ROOT` framework](https://root.cern.ch/). We are going to open it into a `pandas` data frame, using [`uproot`](https://github.com/scikit-hep/uproot) to import the ROOT `TTree`.\n",
    "We also have to define a `key`, which is the name of the `TTree` we want to open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/data_CMS/cms/vernazza/electrons/'\n",
    "fname = dirname + 'ntuple_435.root'\n",
    "key='rechitntupler/hits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = [u'rechit_chip', 'rechit_module', 'rechit_channel',\n",
    "                u'rechit_energy', 'rechit_layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttree = uproot.open(fname)[key]\n",
    "df = ttree.pandas.df(branches, entrystop=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # or df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(level=1,drop=True)\n",
    "df.index.names = ['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have in mind the structure of the data frame, we can get some pandas basics knowledge. Let's say we want to look at all the entries in our `df` where the `rechit_energy` is larger than 20 MIPs. In pandas this can be quickly done with a `query` on the `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we ask for head() because \n",
    "## df[sel] is still a df\n",
    "sel = df['rechit_energy'] > 20\n",
    "df[sel].head() # or df.query('rechit_energy > 20.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have learnt how to apply a boolean selection to a data frame. The `sel` object is a pandas Series made of `True` and `False` entries according to whether the selection (`df['rechit_energy'] > 20`) is satisfied or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple selections and create a new df starting from the original one, but with a selection applied on data. Something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define some selections\n",
    "sel_energy = df['rechit_energy'] > 20\n",
    "sel_layer = df['rechit_layer'] == 12\n",
    "\n",
    "## We combine the selection together.\n",
    "## Note that python uses the single & (or | ).\n",
    "sel_tot = sel_energy & sel_layer\n",
    "\n",
    "## We create a new df with only the events\n",
    "## we are interested on.\n",
    "df_cut = df[sel_tot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the difference in length between these two data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even group items, perform calculations on some aggregated property, then plot values, just to get an idea of larger scale trends. This property of grouping the elements of the data frame together will turn out to be very useful in our analsys. In `pandas` you can `groupby()` a data frame by items. In our case we might be interested at some *per event* information. Hence, we'd `groupby('event')` our `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby('event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gb is an object from which we can get properties on aggregrated items of df\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's say we want to check the min value of \n",
    "## a variable per event. \n",
    "## Here an example for the reconstructed energy:\n",
    "\n",
    "gb.agg({'rechit_energy':'min'}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do combined `groupby` objects. The effect of this will be a grouping over two items. One interesting information in a sampling calorimeter is the longitudinal profile, i.e. the fractional energy deposit per layer. In this way we can have a sort of representation of the longitudinal development of the em shower we are studying. We are going to work on this during our analysis. For now let's try to look at the **hits profile**, meaning the number of hits per layer, per event. When you `groupby` over two items, pandas groups your df for the first item you pass to `groupby` and then it further groups it for the second item: `df.groupby(['item1', 'item2']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When you want to \"count\" some number in a df \n",
    "## (eg number of entries in a row), you can use the .size() method.\n",
    "\n",
    "hits = df.groupby(['event', 'rechit_layer']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `hits` is a pandas `Series` which contains **per event** the information of the number of hits **per layer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualisation is something we have to take care when presenting HEP results. The final plots for your report will have to be clear, self-explanatory and easy to be read. For now, let's quickly plot the longitudinal hits profile for one event. To do this we select **the first entry** of `hits`, which corresponds to a series of hits (per layer) for the event at the 0-th place in the `hits` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_0 = hits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to plot it. To do so, we use the `matplotlib` library we have seen in the tutorial `0_Numpy_Matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hits_0, 'o--')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Hits')\n",
    "plt.title('Hits per layer prof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have just checked how the hits profile in the first event of out df looks like. What about the overall profile? Is there a way to reconstruct the full profile for the 300 GeV positron beam we are looking at? Well, we can take the mean of the hits per layer over all the events. Sounds something familiar, right? Let's `groupby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From `hits` we want to extract the mean\n",
    "## of all the hits in layer 0, 1 ecc:\n",
    "\n",
    "mean_prf = hits.groupby('rechit_layer').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_prf, 'o--')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Hits')\n",
    "plt.title('Hits per layer prof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this profile we can already ask ourself some questions:\n",
    " * Why do we observe a bump for the first layer? To be understood;\n",
    " * Can we get a first hint on where the shower maximum is? \n",
    " * What about the layers? Can we consider a different number of layers?\n",
    " \n",
    "We are going to answer all these questions during our analysis. However it is important to learn this approach to a problem on this kind of data: always try to get a qualitative overall picture. Can you get some features directly from the `df` you have to analyze? Can you answer the questions you have after the first look at the data? ecc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your time to solve this exercises. These questions can help you in learning the analysis tools, in understanding something more about the physics we are going to study and to clarify some questions you might have (or to come up with new ones):\n",
    "\n",
    " * Where would you expect the shower maximum to be? Why?\n",
    " * We are dealing with a 300 GeV positron shower here. What about the calorimeter and its structure: do we really need to consider all the layers as we did above? If not, why?\n",
    " * Can you produce the histogram of the energy sums (per event) distribution? (Hint: you can use `groupby` to get the sum of `rechit_energy` for each `event`)\n",
    " * In the previous notebook we have learnt how to fit. Can you try to fit the distribution of the previous question with a gaussian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
